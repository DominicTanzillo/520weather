Lindsay - Data Acquisition
Before we started actually collecting data, we thought about the pros and cons to the different types of data and time ranges we could collect. We initially were thinking about collecting data from roughly 50 years back to have more data for our model to learn from, thinking that this could decrease the chance that we would overfit based on a smaller pool of data. We looked to Meteostat for weather data since it provides a comprehensive overview of historical weather data, compared to other APIs that focus mostly on more recent weather data. The Meteostat library includes the option to section off the data hours, days and months. We chose to use the hourly class function to get the most precise weather per hour for each day. Until 2013, weather data had some missing values. To minimize NaNs and ensure the reliability of our dataset, we focused our analysis on data collected from 2015-2024, when the data became more consistent without significant gaps. We used a separate csv file to import the data from 2025 before September 17th, 2025, and cut the data to only include September. Then, we used this csv to merge with the rest of our data.
Since we were using data from multiple csv files across different years, the first step was to load and merge all of the datasets into a single DataFrame. We wrote a helper function to read all of the files, normalize the column names (convert the names to lowercase and remove extra spaces) and add them into one large dataset. Once we combined the data, we cleaned the data to account for inconsistent formatting, missing values and outliers. We noticed that some files contained a combined datetime, but others kept their time information as separate columns (i,e., year, month, day, hour). We accounted for this by creating a consistent datetime column. We either parsed an existing datetime column or merged the other four columns into a single timestamp. We then moved the datetime column to the front of the dataset and sorted the entire DataFrame chronologically for better readability. Sorting it chronologically also made sense because weather data is inherently time series data (data points taken as successive equally spaced points in time) and sorting it chronologically also helped us prevent data leakage from future information. We also dropped rows that missed important values like datetime or temp to avoid any reliability issues in our model analysis.
We also dropped low-quality features like the cldc (cloud cover) column, since in class we learned that irrelevant or low-quality features can increase model complexity and potential for overfitting. This helped remove noise and reduce variance, helping our model to generalize better to the unseen predicted dates. We also filled in missing values in the prcp (precipitation) and wdir (wind direction) columns by interpolation; estimating what the missing values should be based on nearby data points. We considered just dropping these features, but decided against it to avoid significant data loss and to maximize our training data. We also tried to limit outliers in our dataset to avoid any distortion in our models  since very large or very small outliers could cause a model to focus too much on those instances, increasing variance. To reduce this risk, we limited the range of the data between the 1st and 99th percentiles, to find which variables were at the very low end and the very high end. Any points outside of this range were adjusted to be equal to the boundary values instead of being left as extreme outliers. This helped our model to focus more on the main trends of the data without being heavily influenced by outliers. Overall, the data cleaning didn’t necessarily help much in making the data look “nicer,” but helped ensure best practices to avoid data leakage, generalization and bias-variance trade-off.



Dominic - Non-Linear Model Ensemble of
For our non-linear model we opted for an ensemble model of three non-linear models which generates an overall non-linear final prediction. The three models we selected are Random Forest Regressor (noted: RF), Gradient Boosting Regression (noted: GB), and Support Vector Regressor (noted: SVR). The overall Ensemble used Voting Regressor which is an average of all three predictions (i.e. we did not assign variable weights). By creating an ensemble we hoped to average our differing features and benefits of each model.

To begin, let’s also clarify what “trees” refer to as our first two models are based around decision trees. A decision tree is effectively a flowchart that moves from parameter to parameter and asks at each feature what the data looks like and then splits based on a series of possible inputs. This decision point is a node and each branching decision is called a branch. This continues across each branch until it reaches the terminal point of a leaf. The leaves and numbers of branches can be modified with hyperparameters that specify the maximum number of branches at each node, the maximum number of nodes between the start/root (tree depth), and maximum total number of terminal leaves. Because the decisions made at each step do not represent a continuous function and there are a total number of terminal leaves that represent discrete values, processes that rely on trees are not considered linear but rather non-linear.

Each Random Forest is a decision tree that bootstraps itself by taking only a subset of data (both test rows and features) to make predictions. By iterating repeatedly, in this case 200, each tree can be a generally underfitted model. By then averaging across all 200 trees, this allows all the features and datapoints to be represented in the final count. Referring to our bias-variance trade-off concept, these random forests in aggregate are more-biased and lower-variance than our other models used and are helpful for a good baseline in our model. Because of the intentional hiding of some of the training data, they are naturally under-fitted even in the aggregate. This data is non-linear because each tree represents a series of decisions/rules that fits the data to different categories and is therefore not a continuous (read: smooth) curve. Averaging across trees smooths this, but does not change it from discrete to continuous.
Gradient Boosting also makes trees, but rather than having a series of unique trees, it continually iterates and updates the trees being constructed with a specified learning rate (alpha, in this case .05) and predicting the residuals of a previous tree to continuously update the model (in this case 200 iterations). This process only applies to the training data and never touches the test data to ensure no data leakage. This process repeatedly looks at our features and attempts to generate a more finely tuned tree to predict our test data. This process is more prone to variance and overfitting with a lower bias because we are repeatedly fine-tuning a tree.
Support Vector Regression attempts to generate a smooth function that has all of the errors within a tolerance. For the space around our function there are different “kernels” possible. This means essentially to determine where our data fits, we can have functions of different dimensions finding space between our points. If we use the radial basis function, it measures distance without assuming dimensions (it can be functionally infinite). This means we have a smooth and non-linear function. It compliments our tree models by emphasizing low-bias and high-variance for a relatively over-fitted function.

Taken in aggregate we have the average of three non-linear models making our final ensemble non-linear.
