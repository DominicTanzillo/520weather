Lindsay - Data Acquisition
Before we started actually collecting data, we thought about the pros and cons to the different types of data and time ranges we could collect. We initially were thinking about collecting data from roughly 50 years back to have more data for our model to learn from, thinking that this could decrease the chance that we would overfit based on a smaller pool of data. We looked to Meteostat for weather data since it provides a comprehensive overview of historical weather data, compared to other APIs that focus mostly on more recent weather data. The Meteostat library includes the option to section off the data hours, days and months. We chose to use the hourly class function to get the most precise weather per hour for each day. Until 2013, weather data had some missing values. To minimize NaNs and ensure the reliability of our dataset, we focused our analysis on data collected from 2015-2024, when the data became more consistent without significant gaps. We used a separate csv file to import the data from 2025 before September 17th, 2025, and cut the data to only include September. Then, we used this csv to merge with the rest of our data.
Since we were using data from multiple csv files across different years, the first step was to load and merge all of the datasets into a single DataFrame. We wrote a helper function to read all of the files, normalize the column names (convert the names to lowercase and remove extra spaces) and add them into one large dataset. Once we combined the data, we cleaned the data to account for inconsistent formatting, missing values and outliers. We noticed that some files contained a combined datetime, but others kept their time information as separate columns (i,e., year, month, day, hour). We accounted for this by creating a consistent datetime column. We either parsed an existing datetime column or merged the other four columns into a single timestamp. We then moved the datetime column to the front of the dataset and sorted the entire DataFrame chronologically for better readability. Sorting it chronologically also made sense because weather data is inherently time series data (data points taken as successive equally spaced points in time) and sorting it chronologically also helped us prevent data leakage from future information. We also dropped rows that missed important values like datetime or temp to avoid any reliability issues in our model analysis.
We also dropped low-quality features like the cldc (cloud cover) column, since in class we learned that irrelevant or low-quality features can increase model complexity and potential for overfitting. This helped remove noise and reduce variance, helping our model to generalize better to the unseen predicted dates. We also filled in missing values in the prcp (precipitation) and wdir (wind direction) columns by interpolation; estimating what the missing values should be based on nearby data points. We considered just dropping these features, but decided against it to avoid significant data loss and to maximize our training data. We also tried to limit outliers in our dataset to avoid any distortion in our models  since very large or very small outliers could cause a model to focus too much on those instances, increasing variance. To reduce this risk, we limited the range of the data between the 1st and 99th percentiles, to find which variables were at the very low end and the very high end. Any points outside of this range were adjusted to be equal to the boundary values instead of being left as extreme outliers. This helped our model to focus more on the main trends of the data without being heavily influenced by outliers. Overall, the data cleaning didn’t necessarily help much in making the data look “nicer,” but helped ensure best practices to avoid data leakage, generalization and bias-variance trade-off.

Jaideep - Linear Model
For our linear model, we opted for a linear regression model. We selected it as it predicts the values by fitting a continuous line through our temperature data. Here, the relationship between inputs and outputs is shown as a weighted sum of features: predicted_temp = intercept + (coef₁ × feature₁) + (coef₂ × feature₂) + ... . The model only uses addition and multiplication, and hence it is easy to interpret, and we can see in a clear way how much a feature affects the prediction just by looking at its coefficient. This makes linear models helpful in understanding what are the most important factors are for predicting the temperatures.

We did some feature engineering to improve our model's performance, we began with the cleaned dataset and added new features to better represent the temperature patterns. The first feature is related to time, as the hours 23 and 0 are numerically closer to each other but not in real life. We used sine and cosine transformation, which creates a circle where hours which are adjacent to each other will stay closer. We did this for both the hour of the day and the day of the year (day 365 and day 1 should be close together). Then the second feature we made is lag feature, here its moves all the values down by 1 row, so the temperature which was in row 5 is in row 6. And row 1 becomes empty. This helps us in giving the model access to recent history, for example, if the temperature was 20°C one hour ago, it's would be around 20°C now. This helps in predicting the temperatures. Similarly, we did it for 24 hours and added a lag, so that the temperature at 3 pm yesterday helps predict the temperature at 3 pm today (ie. daily patterns). We also added a rolling statistics feature - temp_rolling_mean_6h and temp_rolling_mean_24h, which calculates the average temperature for 6 rows that is 6-hour windows and 24-hour windows. This helps in smoothing the short-term fluctuations and helps in revealing the broader trends. The temp_rollong_std_6h measures recent temperature variability and signalys potentially unstable weather.

Before training the final model, we used 5-fold cross-validation on training data to assess stability. Cross-validation helps in splitting the training data into 5 sequential parts and then trains on 4 parts and validates on the remaining part, and it repeats this 5 times. We kept shuffle = False to preserve time series order, since randomly mixing past and future may introduce any leakage. The cross-validation results showed steady performance in every fold, with average MSE between 0.7 and 0.8. This suggests that the model did not overfit. After the validation confirmed the stability, we then trained the final model on all the training data. The resulting model had around features with learned coefficients showing each feature's importance. The temp_rolling_mean_6h had one of the highest coefficient magnitudes (1.81), which indicated that the recent 6-hour average is the strongest predictor. The temp_lag_3h had a large negative coefficient (-0.97), which balances the model by preventing its over dependence on a single recent reading. The cyclical time features had moderate coefficients that capture daily heating and cooling patterns.
In the end, we got MSE = 0.813, RMSE = 0.902°C, and MAE = 0.687°C, which means our model had a good performance. Our model explains about 96% of temperature variation, in relation to the R2 score of 0.958. The linear regression had also performed significantly better during this validation period than the ensemble model, which had an MSE of 11.595. Our linear model is a good weather forecasting model as it has both interpretability and good predictive performance. 


Dominic - Non-Linear Model Ensemble of
For our non-linear model we opted for an ensemble model of three non-linear models which generates an overall non-linear final prediction. The three models we selected are Random Forest Regressor (noted: RF), Gradient Boosting Regression (noted: GB), and Support Vector Regressor (noted: SVR). The overall Ensemble used Voting Regressor which is an average of all three predictions (i.e. we did not assign variable weights). By creating an ensemble we hoped to average our differing features and benefits of each model.

To begin, let’s also clarify what “trees” refer to as our first two models are based around decision trees. A decision tree is effectively a flowchart that moves from parameter to parameter and asks at each feature what the data looks like and then splits based on a series of possible inputs. This decision point is a node and each branching decision is called a branch. This continues across each branch until it reaches the terminal point of a leaf. The leaves and numbers of branches can be modified with hyperparameters that specify the maximum number of branches at each node, the maximum number of nodes between the start/root (tree depth), and maximum total number of terminal leaves. Because the decisions made at each step do not represent a continuous function and there are a total number of terminal leaves that represent discrete values, processes that rely on trees are not considered linear but rather non-linear.

Each Random Forest is a decision tree that bootstraps itself by taking only a subset of data (both test rows and features) to make predictions. By iterating repeatedly, in this case 200, each tree can be a generally underfitted model. By then averaging across all 200 trees, this allows all the features and datapoints to be represented in the final count. Referring to our bias-variance trade-off concept, these random forests in aggregate are more-biased and lower-variance than our other models used and are helpful for a good baseline in our model. Because of the intentional hiding of some of the training data, they are naturally under-fitted even in the aggregate. This data is non-linear because each tree represents a series of decisions/rules that fits the data to different categories and is therefore not a continuous (read: smooth) curve. Averaging across trees smooths this, but does not change it from discrete to continuous.
Gradient Boosting also makes trees, but rather than having a series of unique trees, it continually iterates and updates the trees being constructed with a specified learning rate (alpha, in this case .05) and predicting the residuals of a previous tree to continuously update the model (in this case 200 iterations). This process only applies to the training data and never touches the test data to ensure no data leakage. This process repeatedly looks at our features and attempts to generate a more finely tuned tree to predict our test data. This process is more prone to variance and overfitting with a lower bias because we are repeatedly fine-tuning a tree.
Support Vector Regression attempts to generate a smooth function that has all of the errors within a tolerance. For the space around our function there are different “kernels” possible. This means essentially to determine where our data fits, we can have functions of different dimensions finding space between our points. If we use the radial basis function, it measures distance without assuming dimensions (it can be functionally infinite). This means we have a smooth and non-linear function. It compliments our tree models by emphasizing low-bias and high-variance for a relatively over-fitted function.

Taken in aggregate we have the average of three non-linear models making our final ensemble non-linear.
