{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22d0f94f6aa32bda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7f4291b2367dcbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": "# Predicting Weather from RDU's Weather Station"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72ed6de038e2c607",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "\n",
    "Our task in this exercise is to build the pipeline to convert raw data into features to use in a ML model. The model itself that you will use has already been set up for you (a linear regression model which has been put into a separate script you will import) and **you cannot change the model**, only the data pipeline.\n",
    "\n",
    "## Data\n",
    "You have been given two csv files of data to use in your analysis.  The first file (\"2011-2012_bikes.csv\") contains historical demand data from the past two years of operation. The dataset contains the following columns:\n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- cnt: count of total rental bikes \n",
    "\n",
    "The second file (\"2011-2012_weather.csv\") contains weather information for the same time period.  This dataset contains the following columns:  \n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp : Temperature in Celsius\n",
    "- atemp: Feels-like temperature in Celsius\n",
    "- hum: Humidity\n",
    "- windspeed: Wind speed\n",
    "\n",
    "You may use some or all of the data provided, not all of it is necessarily useful.\n",
    "\n",
    "## Approach\n",
    "Your task in this exercise is to build the pipeline from raw data to features ready for modeling.  There are many possible approaches to doing this, some are better, some are worse.  \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caaa3b69ecda13df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-10-06T18:21:55.802895Z",
     "start_time": "2025-10-06T18:21:55.602888Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.create_combined import load_and_clean_weather\n",
    "\n",
    "df = load_and_clean_weather()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 72306-2015.csv ...\n",
      "Reading 72306-2016.csv ...\n",
      "Reading 72306-2017.csv ...\n",
      "Reading 72306-2018.csv ...\n",
      "Reading 72306-2019.csv ...\n",
      "Reading 72306-2020.csv ...\n",
      "Reading 72306-2021.csv ...\n",
      "Reading 72306-2022.csv ...\n",
      "Reading 72306-2023.csv ...\n",
      "Reading 72306-2024.csv ...\n",
      "Reading 72306-2025.csv ...\n",
      "Final dataset: 94599 rows, 10 columns\n",
      "Date range: 2015-01-01 00:00:00 → 2025-10-16 15:00:00\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T17:04:07.875701Z",
     "start_time": "2025-10-06T17:04:07.122753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data_clean import clean_data\n",
    "\n",
    "output = clean_data(df)\n",
    "\n",
    "os.makedirs(\"Data_Cleaned\", exist_ok=True)\n",
    "output.to_csv(\"Data_Cleaned/cleaned_weather_final.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 72306-2015.csv ...\n",
      "Reading 72306-2016.csv ...\n",
      "Reading 72306-2017.csv ...\n",
      "Reading 72306-2018.csv ...\n",
      "Reading 72306-2019.csv ...\n",
      "Reading 72306-2020.csv ...\n",
      "Reading 72306-2021.csv ...\n",
      "Reading 72306-2022.csv ...\n",
      "Reading 72306-2023.csv ...\n",
      "Reading 72306-2024.csv ...\n",
      "Reading 72306-2025.csv ...\n",
      "\n",
      "Combined dataset shape: 94599 rows, 22 columns.\n",
      "Columns found: ['year', 'month', 'day', 'hour', 'temp', 'temp_source', 'rhum', 'rhum_source', 'prcp', 'prcp_source', 'wdir', 'wdir_source', 'wspd', 'wspd_source', 'wpgt', 'wpgt_source', 'pres', 'pres_source', 'cldc', 'cldc_source', 'coco', 'coco_source']\n",
      "\n",
      "---- Cleaning Data ----\n",
      "Datetime column built and moved to front.\n",
      "Dropped 'cldc' column due to high missingness.\n",
      "Interpolated 'prcp': 6400 → 352 missing values.\n",
      "Interpolated 'wdir': 1115 → 0 missing values.\n",
      "Cleaned dataset has 94599 rows and 18 columns.\n",
      "-----------------------\n",
      "Sanity check: dropped 94599 rows with remaining NaN values.\n",
      "\n",
      "Preview of cleaned data:\n",
      "Empty DataFrame\n",
      "Columns: [datetime, temp, temp_source, rhum, rhum_source, prcp, prcp_source, wdir, wdir_source, wspd, wspd_source, wpgt, wpgt_source, pres, pres_source, cldc_source, coco, coco_source]\n",
      "Index: []\n",
      "\n",
      "---- Cleaning Data ----\n",
      "Datetime column built and moved to front.\n",
      "Dropped 'cldc' column due to high missingness.\n",
      "Interpolated 'prcp': 6400 → 352 missing values.\n",
      "Interpolated 'wdir': 1115 → 0 missing values.\n",
      "Cleaned dataset has 94599 rows and 9 columns.\n",
      "-----------------------\n",
      "Sanity check: dropped 352 rows with remaining NaN values.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81357d8c0527b793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-06T18:21:57.492225Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from src.ensemble.model_creation import run_pipeline\n",
    "\n",
    "# Case 1: If Sept 2025 labels exist in your cleaned_weather.csv\n",
    "X_train, y_train, X_test, y_test, model, y_pred = run_pipeline(\n",
    "    weather_filename=\"Data_Cleaned/cleaned_weather_final.csv\",\n",
    "    forecast_start=\"2025-09-17\",\n",
    "    forecast_end=\"2025-09-30\"\n",
    ")\n",
    "\n",
    "# Case 2: If you later only want to forecast (no labels in data)\n",
    "_, _, X_future, _, trained_model, future_pred = run_pipeline(\n",
    "    weather_filename=\"Data_Cleaned/cleaned_weather_final.csv\",\n",
    "    forecast_start=\"2025-09-17\",\n",
    "    forecast_end=\"2025-09-30\"\n",
    ")\n",
    "print(\"First 10 predictions:\", future_pred[:10])\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test.values, label=\"Actual\", linewidth=2)\n",
    "plt.plot(y_pred, label=\"Predicted\", linewidth=2)\n",
    "plt.title(\"Forecast: Sept 17–30, 2025\")\n",
    "plt.xlabel(\"Time Index (Sept 17–30)\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ensemble...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efcc307a64ed278a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load data\n",
    "Create and run a function `load_data()` to do your data loading and any merging needed.  You can specify the arguments and returns as needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b80b9bb85d4194ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.944766Z",
     "start_time": "2025-09-17T15:23:23.942648Z"
    }
   },
   "source": [
    "def load_data(file1, file2):\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    merged = pd.merge(df1, df2, how=\"outer\", on=['dteday','hr'])\n",
    "    return merged"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66c03a03105ee793",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Clean data\n",
    "Use the cell below to create and run a function `clean_data()` which cleans up the data as needed.  Things you may want to consider at this stage include:  \n",
    "- Checking for and handling any missing values \n",
    "- Identifying any erroneous data and handling \n",
    "- Identifying outliers and determining whether to remove/adjust them or leave them as-is\n",
    "- Etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c4638c5ecf84143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.952704Z",
     "start_time": "2025-09-17T15:23:23.948862Z"
    }
   },
   "source": [
    "def clean_data(s1):\n",
    "    # Standardize dates\n",
    "    s1['dteday'] = pd.to_datetime(s1['dteday'], errors='coerce')\n",
    "\n",
    "    # Convert to epoch seconds\n",
    "    s1['dteday'] = (s1['dteday'] + pd.to_timedelta(s1['hr'], unit='h')).astype('int64') // 10**9\n",
    "\n",
    "    # Forcing Data into numeric if not already\n",
    "    for col in s1.columns[1:]:\n",
    "        s1[col] = pd.to_numeric(s1[col], errors='coerce')\n",
    "\n",
    "    # Dropping non-numerics\n",
    "    s2 = s1.dropna()\n",
    "\n",
    "    print(\"Dropped \" + str(len(s1)-len(s2)) + \" rows from our dataset due to missing values.\")\n",
    "\n",
    "    # Remove extrema\n",
    "    numeric_cols = s2.iloc[:, 2:]\n",
    "    s3 = s2.copy()\n",
    "\n",
    "    for col in numeric_cols.columns:\n",
    "        # Get the Series\n",
    "        series = s2[col]\n",
    "\n",
    "        # Calculate mean and std\n",
    "        col_mn = series.mean()\n",
    "        col_st = series.std()\n",
    "\n",
    "        # Mask rows within 2 std\n",
    "        keep_col = (series > col_mn - 2 * col_st) & (series < col_mn + 2 * col_st)\n",
    "\n",
    "        # Drop rows outside this range\n",
    "        s3 = s3[keep_col]\n",
    "    print(\"Dropped \" +str(len(s2)-len(s3))+ \" columns due to extrema\")\n",
    "\n",
    "    s4 = s3.drop(columns=['hr'])\n",
    "\n",
    "    return s4"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f15ec71cf3a1a85a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Split data for training and testing\n",
    "Create and run the function `split_data()` in the cell below to split the data into training and test sets.  You should use all data up to and including July 31 2012 as the training set, and the data for the period August 1 2012 - December 31 2012 as the test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-578054bc7e2183d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.958281Z",
     "start_time": "2025-09-17T15:23:23.956168Z"
    }
   },
   "source": [
    "def split_data(s1):\n",
    "    # Define cutoff\n",
    "    cutoff = int(pd.to_datetime(\"2012-08-01 00:00:00\").timestamp())\n",
    "\n",
    "    before = s1[s1['dteday'] <= cutoff]\n",
    "    after  = s1[s1['dteday'] > cutoff]\n",
    "\n",
    "    target = 'cnt'\n",
    "\n",
    "    # Split into X and y\n",
    "    X_train = before.drop(columns=[target])\n",
    "    y_train = before[target]\n",
    "\n",
    "    X_test = after.drop(columns=[target])\n",
    "    y_test = after[target]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Create and run the function `build_features()` below to create any additional derivative features (e.g. time series features) that you wish to use in modeling.  You will need to apply this function to both your training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.962598Z",
     "start_time": "2025-09-17T15:23:23.961127Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c341995cddfacb2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Feature Selection\n",
    "Use the cell below to create and run the function `feature_select()` which performs feature selection using univariate (filter) methods.  After you analyze the correlations, determine whether you would like to remove any features and do so."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d3a200b6fa58a74",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.966329Z",
     "start_time": "2025-09-17T15:23:23.965199Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-09ef0b02826676d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Prepare Features for Modeling\n",
    "Our final step in the pipeline is to prepare our feature set for modeling.  In particular, in this step we need to ensure that any categorical variables we may be using are encoded as numeric values in order for the model to function properly.  You might also consider scaling some of your data.\n",
    "\n",
    "In the below cell create and run a function `prepare_train_feats()` which prepares the training features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff64eead1e1344c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.969548Z",
     "start_time": "2025-09-17T15:23:23.968433Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-051c3772cf091103",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We also need to prepare the features in our test set in the same way to feed into the model.  Use the cell below for the function `prepare_test_feats()` which prepares your test set features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb2f6842a35928a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.972636Z",
     "start_time": "2025-09-17T15:23:23.971570Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd9d8e5013253357",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run pipeline\n",
    "Finally, let's bring everything together in a function to run the entire pipeline for our training data.  Complete the function `run_pipeline()` in the cell below.  The function should call any/all of the functions you have defined above which are needed to load the data, transform it and prepare the features for both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f914f1eb4902eff1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:23.976757Z",
     "start_time": "2025-09-17T15:23:23.974943Z"
    }
   },
   "source": [
    "def run_pipeline(bike_filename, weather_filename):\n",
    "    '''\n",
    "    Runs your pipeline (calling the above functions as needed) to transform the raw data into the training and test data sets for modeling\n",
    "\n",
    "    Inputs:\n",
    "        bike_filename(str): name of the file containing the bike data\n",
    "        weather_filename(str): name of the file containing the weather data\n",
    "\n",
    "    Returns:\n",
    "        X_train(pd.DataFrame): dataframe containing the training set inputs\n",
    "        y_train(pd.DataFrame): dataframe containing the training set labels\n",
    "        X_test(pd.DataFrame): dataframe containing the test set inputs\n",
    "        y_test(pd.DataFrame): dataframe containing the test set labels\n",
    "    '''\n",
    "    merged = load_data(bike_filename,weather_filename)\n",
    "    clean = clean_data(merged)\n",
    "    X_train, y_train, X_test, y_test = split_data(clean)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-278cf4c4ab73ce46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we've prepared our features we are ready to run our model.  Run the cell below, which trains the model on the training set and calculates and reports the mean squared error (MSE) on the test set.  If everything went well you should have a MSE below 18500"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d84dda8b73e82cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2025-09-17T15:23:24.020714Z",
     "start_time": "2025-09-17T15:23:23.981255Z"
    }
   },
   "source": [
    "bike_datafile = \"2011-2012_bikes.csv\"\n",
    "weather_datafile = \"2011-2012_weather_messy.csv\"\n",
    "X_train, y_train, X_test, y_test = run_pipeline(bike_datafile, weather_datafile)\n",
    "mse_score = run_model(X_train, y_train, X_test, y_test)\n",
    "print('Mean Squared Error on the test set: {:.2f}'.format(mse_score))\n",
    "\n",
    "assert mse_score < 18500"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 58 rows from our dataset due to missing values.\n",
      "Dropped 3416 columns due to extrema\n",
      "Mean Squared Error on the test set: 17888.35\n"
     ]
    }
   ],
   "execution_count": 85
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "aipi520",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
